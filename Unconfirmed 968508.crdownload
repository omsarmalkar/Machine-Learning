#!/usr/bin/env python
# coding: utf-8

# 
# # Linear Regression Practice
# <div class="alert alert-block alert-info">
# This is just a sample I picked from GITHUB to practice a regression model with scikitlearn. 
# 
# > Data set has 985 Rows and 12 Columns.
# 
# > Goal is to identify variables impacting price of the house in Secremento
# 
# 
# 

# In[1]:


get_ipython().run_line_magic('matplotlib', 'inline')
import matplotlib.pyplot as plt
import numpy as np
import sympy as sy
import pandas as pd
import pandas_profiling
import seaborn as sns
import mpl_toolkits
from sklearn.linear_model import LinearRegression
from sklearn.dummy import DummyRegressor
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error
from sklearn import metrics

import warnings
warnings.filterwarnings('ignore')


# In[2]:


get_ipython().run_cell_magic('HTML', '', '<style type="text/css">\n    table.dataframe td, table.dataframe th {\n        border-style: solid;\n        border: 3px solid lightgray;\n    }\n</style>')


# In[35]:


data = pd.read_csv('https://raw.githubusercontent.com/jfkoehler/linear-regression-video/master/data/sacremento_housing.csv', index_col=0)
data.head()


# In[36]:


data.shape


# In[37]:


data.info()


# In[38]:


data.sample(10)


# In[39]:


data.describe(include = 'all')  


# In[40]:


data.city.value_counts().plot(kind='bar',figsize=(20,8))


# In[20]:


pandas_profiling.ProfileReport(data)


# # Initial Observations
# <div class="alert alert-block alert-warning">
#     
# - Removal of duplicate rows 
# - Drop rows with latitude,	longitude, state, street, sale_date, zip,type
# - Zeros in data for Bedroom & Bathroom is justified as, there would be houses which will not have bathroom or bedroom (e.g. Condos) at time such houses will be having low price valuation compared to other ones (Quick analysis can justify, if not can be replaced by Mean)
# - Square Feet cannot be zero, can be replaced with median value
# - Price cannot be negative

# **Dropping duplicate rows**

# In[41]:


data = data.drop_duplicates(subset=None, keep='first', inplace=False)


# **Dropping few columns which will not impact our analysis**

# In[42]:


data = data.drop(['latitude', 'longitude', 'state', 'street', 'sale_date', 'zip','type'], axis=1)


# In[43]:


data.rename(columns={'sq__ft':'sqft'}, inplace=True)
 
print(data.columns)


# **Quick check on missing values**

# In[44]:


def missing_data(data):
    total = data.isnull().sum().sort_values(ascending = False)
    percent = (data.isnull().sum()/data.isnull().count()*100).sort_values(ascending = False)
    return pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])
missing_data(data)


# In[45]:


data.sample(5)


# **Removing bath/beds with zero values**

# In[46]:


data1 = data[data['city']== 'SACRAMENTO']     #Filter SACRAMENTO


# In[47]:


data1 = data1[data1['baths']!= 0]    #Removing zeros in bathroom and bedroom
data1 = data1[data1['beds']!= 0] 


# In[48]:


data1.baths.unique()


# In[49]:


data1.beds.unique()


# In[50]:


data1['sqft']=data1['sqft'].replace(0,data1['sqft'].median())


# In[51]:


data1 = data1[data1['price']>= 0]  


# In[52]:


data1.info()


# In[53]:


data1.sample(10)


# In[55]:


data_s = data1[['price','sqft','beds','baths']]


# In[56]:


pandas_profiling.ProfileReport(data_s)


# # Hurraayyy!! data is clean now ready for EDA & Regression

# In[57]:


data_s.info()


# In[58]:


data_s.sample()


# In[59]:


data_s['beds'].value_counts().plot(kind='bar')
plt.title('number of Bedroom')
plt.xlabel('Bedrooms')
plt.ylabel('Count')
sns.despine


# In[60]:


plt.scatter(data_s.price,data_s.sqft)
plt.title("Price vs Square Feet")


# In[61]:


sns.catplot("beds", "price", data=data_s, kind="bar" ,palette="PuBuGn_d",height=6, aspect=2)
plt.xlabel('Bed Room')
plt.ylabel('Price')
plt.show()


# **Something fishy with 6 & 8 bed rooms cant be so cheap, lets filter out as counts are negligible**

# In[62]:


data_s = data_s[data_s['beds']!= 6]   
data_s = data_s[data_s['beds']!= 8]   


# In[63]:


data_s.beds.unique()


# In[64]:


data_r = data_s.copy(deep=True)           #ready for regression          


# In[66]:


data_r.sample(5)


# # Regression

# In[67]:


data_r.describe()


# In[68]:


f, axes = plt.subplots(2, 2, figsize=(7, 7), sharex=True)                                      # Set up the matplotlib figure
sns.despine(left=True)

sns.distplot(data_r.price, color="b", ax=axes[0, 0])

sns.distplot(data_r.sqft, color="r", ax=axes[0, 1])

sns.distplot(data_r.beds, color="g", ax=axes[1, 0])

sns.distplot(data_r.baths, color="m", ax=axes[1, 1])


# In[69]:


JG1 = sns.jointplot("sqft", "price", data=data_r, kind='reg')
JG2 = sns.jointplot("beds", "price", data=data_r, kind='reg')
JG3 = sns.jointplot("baths", "price", data=data_r, kind='reg')

#subplots migration
f = plt.figure()
for J in [JG1, JG2,JG3]:
    for A in J.fig.axes:
        f._axstack.add(f._make_key(A), A)


# In[53]:


sns.pairplot(data_r, size = 2, aspect = 1.5)


# In[70]:


sns.pairplot(data, x_vars=['sqft', 'beds', 'baths'], y_vars='price', size=5, aspect=1, kind='reg')


# __Observation__
# 
# - Strong relationship between sqft & price
# - Weak relationship between beds and baths

# In[71]:


data_s.corr()


# In[72]:


sns.heatmap( data_s.corr(), annot=True );


# __No Observations from correlation__

# ## Introduction to Linear Regression

# __Linear regression__ is a _basic_ and _commonly_ used type of __predictive analysis__.  The overall idea of regression is to examine two things: 
# - Does a set of __predictor variables__ do a good job in predicting an __outcome__ (dependent) variable?  
# - Which variables in particular are __significant predictors__ of the outcome variable, and in what way they do __impact__ the outcome variable?  
# 
# These regression estimates are used to explain the __relationship between one dependent variable and one or more independent variables__.  The simplest form of the regression equation with one dependent and one independent variable is defined by the formula :<br/>
# $y = \beta_0 + \beta_1x$
# 
# ![image.png](attachment:image.png)
# 
# What does each term represent?
# - $y$ is the response
# - $x$ is the feature
# - $\beta_0$ is the intercept
# - $\beta_1$ is the coefficient for x
# 
# 
# Three major uses for __regression analysis__ are: 
# - determining the __strength__ of predictors,
#     - Typical questions are what is the strength of __relationship__ between _dose and effect_, _sales and marketing spending_, or _age and income_.
# - __forecasting__ an effect, and
#     - how much __additional sales income__ do I get for each additional $1000 spent on marketing?
# - __trend__ forecasting.
#     - what will the __price of house__ be in _6 months_?

# ### Linear Regression Equation with Errors in consideration

# While taking errors into consideration the equation of linear regression is: 
# Generally speaking, coefficients are estimated using the **least squares criterion**, which means we are find the line (mathematically) which minimizes the **sum of squared residuals** (or "sum of squared errors"):
# 

# How do the model coefficients relate to the least squares line?
# - $\beta_0$ is the **intercept** (the value of $y$ when $x$ = 0)
# - $\beta_1$ is the **slope** (the change in $y$ divided by change in $x$)
# 
# Here is a graphical depiction of those calculations:

# ### Assumptions of Linear Regression
# 1. There should be a linear and additive relationship between dependent (response) variable and independent (predictor) variable(s). A linear relationship suggests that a change in response Y due to one unit change in X¹ is constant, regardless of the value of X¹. An additive relationship suggests that the effect of X¹ on Y is independent of other variables.
# 2. There should be no correlation between the residual (error) terms. Absence of this phenomenon is known as Autocorrelation.
# 3. The independent variables should not be correlated. Absence of this phenomenon is known as multicollinearity.
# 4. The error terms must have constant variance. This phenomenon is known as homoskedasticity. The presence of non-constant variance is referred to heteroskedasticity.
# 5. The error terms must be normally distributed.

#  **Preparing X and y using pandas**

# - __Standardization__. <br/>
# Standardize features by removing the _mean_ and scaling to _unit standard deviation_.

# In[142]:


from sklearn.preprocessing import StandardScaler
scaler = StandardScaler().fit(data_s)
dataR = scaler.transform(data_s)


# In[143]:


dataR = pd.DataFrame(dataR)
dataR.head()


# In[144]:


dataR.columns = ['price','sqft','bed','bath']
dataR.head()


# In[145]:


dataR.info()


# In[146]:


feature_cols = ['sqft', 'bed', 'bath']                # create a Python list of feature names
X = dataR[feature_cols]  


# In[147]:


# Splitting the dataset into training and test sets.
data_train, data_test = train_test_split(dataR, test_size = 0.2, random_state = 0)


# In[164]:


# Dropping price from x_train and x_test matrices, and creating y_train and y_test vectors for price values.
x_train = data_train.drop(['price'], 1)
y_train = data_train['price']
x_test = data_test.drop(['price'], 1)
y_test = data_test['price']


# In[149]:


from sklearn.model_selection import train_test_split

def split(X,y):
    return train_test_split(X, y, test_size=0.20, random_state=1)


# In[150]:


# Checking the shapes of training and test sets.
print('Shape of x_train: ', x_train.shape)
print('Shape of y_train: ', y_train.shape)
print('Shape of x_test: ', x_test.shape)
print('Shape of y_test: ', y_test.shape)


# In[151]:


linreg = LinearRegression()
linreg.fit(x_train, y_train)


# In[152]:


y_pred_train = linreg.predict(x_train)


# In[153]:


y_pred_train[:10]


# In[154]:


y_pred_test = linreg.predict(x_test)


# In[155]:


y_pred_test[:10]


# ## Linear regression in scikit-learn

# To apply any machine learning algorithm on your dataset, basically there are 4 steps:
# 1. Load the algorithm
# 2. Instantiate and Fit the model to the training dataset
# 3. Prediction on the test set
# 4. Calculating Root mean square error 
# The code block given below shows how these steps are carried out:<br/>
# 
# ``` from sklearn.linear_model import LinearRegression
#     linreg = LinearRegression()
#     linreg.fit(X_train, y_train) 
#     RMSE_test = np.sqrt(metrics.mean_squared_error(y_test, y_pred_test))```

# In[166]:


X_train, X_test, y_train, y_test=split(X,y)
print('Train cases as below')
print('X_train shape: ',X_train.shape)
print('y_train shape: ',y_train.shape)
print('\nTest cases as below')
print('X_test shape: ',X_test.shape)
print('y_test shape: ',y_test.shape)


# In[157]:


def linear_reg( X, y, gridsearch = False):
    
    X_train, X_test, y_train, y_test = split(X,y)
    
    from sklearn.linear_model import LinearRegression
    linreg = LinearRegression()
    
    if not(gridsearch):        
        linreg.fit(X_train, y_train) 

    else:
        from sklearn.model_selection import GridSearchCV
        parameters = {'normalize':[True,False], 'copy_X':[True, False]}
        linreg = GridSearchCV(linreg,parameters, cv = 10,refit = True)
        linreg.fit(X_train, y_train)                                                           # fit the model to the training data (learn the coefficients)
        print("Mean cross-validated score of the best_estimator : ", linreg.best_score_)  
        
    y_pred_test = linreg.predict(X_test)                                                   # make predictions on the testing set

    RMSE_test = (metrics.mean_squared_error(y_test, y_pred_test))                          # compute the RMSE of our predictions
    print('RMSE for the test set is {}'.format(RMSE_test))

    return linreg


# ### Linear Regression Model without GridSearcCV
# Note:  Linear Regression Model with GridSearcCV is implemented at Table of Contents: 8

# In[158]:


X = dataR[feature_cols]  
y = dataR.price
linreg = linear_reg(X,y)


# ### Interpreting Model Coefficients

# In[159]:


print('Intercept:',linreg.intercept_)          # print the intercept 
print('Coefficients:',linreg.coef_)  


# In[160]:


feature_cols.insert(0,'Intercept')
coef = linreg.coef_.tolist()            
coef.insert(0, linreg.intercept_)   


# In[161]:


eq1 = zip(feature_cols, coef)

for c1,c2 in eq1:
    print(c1,c2)


# __Y = 0.01392 + (0.9193686 `*` SquareFoot) + (-0.22929 `*` Bedrooms) + (-0.073774 `*` Bathrooms)__

# **We interpret the SquareFoot coefficient (0.9193686)**
# 
# **Price of the house is dependant on size of the house**

# ## Using the Model for Prediction

# In[167]:


y_pred_train = linreg.predict(X_train)  
y_pred_train


# In[168]:


y_pred_test = linreg.predict(X_test)                                                           # make predictions on the testing set
y_pred_test


# - We need an evaluation metric in order to compare our predictions with the actual values.

# ## Model evaluation 

# __Error__ is the _deviation_ of the values _predicted_ by the model with the _true_ values.<br/>
# For example, if a model predicts that the price of apple is Rs75/kg, but the actual price of apple is Rs100/kg, then the error in prediction will be Rs25/kg.<br/>
# Below are the types of error we will be calculating for our _linear regression model_:
# - Mean Absolute Error
# - Mean Squared Error
# - Root Mean Squared Error

# ### Model Evaluation using metrics

# __Mean Absolute Error__ (MAE) is the mean of the absolute value of the errors:
# $$\frac 1n\sum_{i=1}^n|y_i-\hat{y}_i|$$
# Computing the MAE for our Pirce predictions

# In[171]:


MAE_train = metrics.mean_absolute_error(y_train, y_pred_train)
MAE_test = metrics.mean_absolute_error(y_test, y_pred_test)


# In[172]:


print('MAE for training set is {}'.format(MAE_train))
print('MAE for test set is {}'.format(MAE_test))


# __Mean Squared Error__ (MSE) is the mean of the squared errors:
# $$\frac 1n\sum_{i=1}^n(y_i-\hat{y}_i)^2$$
# 
# Computing the MSE for our Price predictions

# In[175]:


MSE_train = metrics.mean_squared_error(y_train, y_pred_train)
MSE_test = metrics.mean_squared_error(y_test, y_pred_test)


# In[176]:


print('MSE for training set is {}'.format(MSE_train))
print('MSE for test set is {}'.format(MSE_test))


# __Root Mean Squared Error__ (RMSE) is the square root of the mean of the squared errors:
# 
# $$\sqrt{\frac 1n\sum_{i=1}^n(y_i-\hat{y}_i)^2}$$
# 
# Computing the RMSE for our Price predictions

# In[178]:


RMSE_train = np.sqrt( metrics.mean_squared_error(y_train, y_pred_train))
RMSE_test = np.sqrt(metrics.mean_squared_error(y_test, y_pred_test))


# In[179]:


print('RMSE for training set is {}'.format(RMSE_train))
print('RMSE for test set is {}'.format(RMSE_test))


# Comparing these metrics:
# 
# - __MAE__ is the easiest to understand, because it's the __average error.__ 
# - __MSE__ is more popular than MAE, because MSE "punishes" larger errors.
# - __RMSE__ is even more popular than MSE, because RMSE is _interpretable_ in the "y" units.
#     - Easier to put in context as it's the same units as our response variable.

# ## Model Evaluation using Rsquared value.

# - There is one more method to evaluate linear regression model and that is by using the __Rsquared__ value.<br/>
# - R-squared is the **proportion of variance explained**, meaning the proportion of variance in the observed data that is explained by the model, or the reduction in error over the **null model**. (The null model just predicts the mean of the observed response, and thus it has an intercept and no slope.)
# 
# - R-squared is between 0 and 1, and higher is better because it means that more variance is explained by the model. But there is one shortcoming of Rsquare method and that is **R-squared will always increase as you add more features to the model**, even if they are unrelated to the response. Thus, selecting the model with the highest R-squared is not a reliable approach for choosing the best linear model.
# 
# There is alternative to R-squared called **adjusted R-squared** that penalizes model complexity (to control for overfitting).

# In[180]:


yhat = linreg.predict(X_train)
SS_Residual = sum((y_train-yhat)**2)
SS_Total = sum((y_train-np.mean(y_train))**2)
r_squared = 1 - (float(SS_Residual))/SS_Total
adjusted_r_squared = 1 - (1-r_squared)*(len(y_train)-1)/(len(y_train)-X_train.shape[1]-1)
print(r_squared, adjusted_r_squared)


# In[181]:


yhat = linreg.predict(X_test)
SS_Residual = sum((y_test-yhat)**2)
SS_Total = sum((y_test-np.mean(y_test))**2)
r_squared = 1 - (float(SS_Residual))/SS_Total
adjusted_r_squared = 1 - (1-r_squared)*(len(y_test)-1)/(len(y_test)-X_test.shape[1]-1)
print(r_squared, adjusted_r_squared)


# ## Feature Selection
# 
# 
# At times some features do not contribute much to the accuracy of the model, in that case its better to discard those features.<br/> 
# - Let's check whether __"Bath or Beds"__ improve the quality of our predictions or not.<br/> 
# Lets do some trial and error on features and see if the error (RMSE) is reducing or not.
# - Also Applying __Gridsearch__ method for exhaustive search over specified parameter values of  estimator.

# In[185]:


feature_cols = ['sqft','bath']                                                          # create a Python list of feature names
X = dataR[feature_cols]  
y = dataR.price
linreg=linear_reg(X,y, gridsearch=True)


# In[186]:


feature_cols = ['sqft','bed']                                                          # create a Python list of feature names
X = dataR[feature_cols]  
y = dataR.price
linreg=linear_reg(X,y, gridsearch=True)


# ### With feature selection Bathroom has better RMSE with squarefoot

# # THE END

# <div class="alert alert-block alert-warning">
# **Feedback if any please email omsarmalkar@gmail.com
